// Copyright 2013 Marc-Antoine Ruel. All rights reserved.
// Use of this source code is governed by the Apache v2.0 license that can be
// found in the LICENSE file.

package server

// This module defines all the HTTP handlers the isolate server supports.

import (
	"bytes"
	"compress/zlib"
	"crypto/hmac"
	"crypto/sha1"
	"crypto/subtle"
	"encoding/hex"
	"encoding/json"
	"fmt"
	gorillaContext "github.com/gorilla/context"
	"github.com/gorilla/mux"
	"github.com/maruel/aedmz"
	"hash"
	"io"
	"io/ioutil"
	"net/http"
	"net/url"
	"regexp"
	"strconv"
	"strings"
	"time"
)

type contextKeyType int

const (
	// IsolateProtocolVersion is the current protocol version.
	IsolateProtocolVersion = "1.0"

	// DefaultLinkExpiration is the default expiration time for signed Google
	// Cloud Storage links.
	DefaultLinkExpiration = 4 * time.Hour

	routerKey contextKeyType = 0
)

// ContentNamespace is used as an ancestor of ContentEntry to create mutiple
// content-addressed "tables".
//
// Eventually, the table name could have a prefix to determine the hashing
// algorithm, like 'sha1-'.
//
// There's usually only one table name:
// - default:    The default CAD.
// - temporary*: This family of namespace is a discardable namespace for testing
//               purpose only.
//
// The table name can have suffix:
// - -gzip or -deflate: The namespace contains the content in deflated format.
//                      The content key is the hash of the uncompressed data, not
//                      the compressed one. That is why it is in a separate
//                      namespace.
//
// All the tables in the temporary* family must have is_testing==True and the
// others is_testing==False.
type ContentNamespace struct {
	IsTesting bool      `datastore:"is_testing"`
	Creation  time.Time `datastore:"creation,noindex"` // auto_now=True
}

// ContentEntry represents the content, keyed by its SHA-1 hash, for a single
// entry.
type ContentEntry struct {
	// The GS filename. blobstore.create_upload_url() doesn't permit specifying a
	// reliable filename, so save the autogenerated filename. Save the file size
	// too.
	GSFilename string `datastore:"filename,noindex"`

	// Cache the file size for statistics purposes.
	Size int64 `datastore:"size"`

	// Serves two purposes. It is only set once the Entry had its hash been
	// verified. -1 means it wasn't verified yet.
	//
	// The value is the Cache the expanded file size for statistics purposes. Its
	// value is different from size only in compressed namespaces.
	ExpandedSize int64 `datastore:"expanded_size"`

	// The content stored inline. This is only valid if the content was smaller
	// than MinSizeForGS.
	Content []byte `datastore:"content"`

	// The day the content was last accessed. This is used to determine when
	// data is old and should be cleared.
	LastAccess time.Time `datastore:"last_access"`

	Creation time.Time `datastore:"creation,noindex"`

	// It is an .isolated file.
	IsIsolated bool `datastore:"is_isolated"`
}

// isCompressed return true if the raw data wad modified in any form, e.g.
// compressed, so that the SHA-1 doesn't match.
func (c *ContentEntry) isCompressed(key *aedmz.Key) bool {
	id := key.Parent.StringID
	return strings.HasSuffix(id, "-gzip") || strings.HasSuffix(id, "-deflate")
}

// createEntry generates a new ContentEntry from the request if one doesn't
// exist.
//
// Creates the ContentNamespace entity on the fly if needed.
//
// Returns None if there is a problem generating the entry or if an entry already
// exists with the given hex encoded SHA-1 hash |hash_key|.
func createEntry(c aedmz.RequestContext, namespace, hashKey string) (*ContentEntry, error) {
	length := getHash(namespace).Size() * 2
	if ok, _ := regexp.MatchString(fmt.Sprintf("^[a-f0-9]{%d}$", length), hashKey); !ok {
		return nil, fmt.Errorf("given an invalid key: %s", hashKey)
	}

	k := aedmz.NewKey("ContentNamespace", namespace, nil)
	n := &ContentNamespace{strings.HasPrefix(namespace, "temporary"), time.Now().UTC()}
	if err := aedmz.Get(c, k, n); err != nil {
		if _, err = aedmz.Put(c, k, n); err != nil {
			return nil, err
		}
	}
	// The entity was not present. Create a new one.
	return &ContentEntry{
		LastAccess:   time.Now().UTC(),
		Creation:     time.Now().UTC(),
		ExpandedSize: -1,
	}, nil
}

// gsFilepath returns the full path of an object saved in GS.
func (c *ContentEntry) gsFilepath(key *aedmz.Key) string {
	if c.Content == nil {
		return ""
	}
	// namespace/hash_key.
	return fmt.Sprintf("%s/%s", key.Parent.StringID, c.GSFilename)
}

func (c *ContentEntry) purge(r aedmz.RequestContext, key *aedmz.Key, bucket string) error {
	err := aedmz.Delete(r, key)
	if err != nil {
		return err
	}
	// The key is also the file name.
	return deleteFile(r, bucket, key.StringID)
}

// Utilities.

// getHash returns the hashing algorithm for this namespace.
func getHash(namespace string) hash.Hash {
	// TODO(maruel): Support other algos.
	return sha1.New()
}

// getExpander returns the compression algorithm for this namespace.
func getExpander(r io.ReadCloser, namespace string) (io.ReadCloser, error) {
	if strings.HasSuffix(namespace, "-deflate") || strings.HasSuffix(namespace, "-gzip") {
		return zlib.NewReader(r)
	}
	// Not compressed.
	return r, nil
}

// expandAndHashContent returns the hex encoded hash and expanded content size.
func expandAndHashContent(r io.ReadCloser, namespace string) (string, int64, error) {
	h := getHash(namespace)
	e, err := getExpander(r, namespace)
	if err != nil {
		return "", 0, err
	}
	defer func() {
		_ = e.Close()
	}()
	count, err := io.Copy(h, e)
	return hex.EncodeToString(h.Sum(nil)), count, err
}

// Handlers.

// Verify the SHA-1 matches for an object stored in Cloud Storage.
func internalVerifyWorkerHandler(w http.ResponseWriter, r *http.Request) {
	//if not self.request.headers.get('X-AppEngine-QueueName'):
	//  self.abort(405, detail='Only internal task queue tasks can do this')
	vars := mux.Vars(r)
	namespace := vars["namespace"]
	hashKey := vars["hashkey"]

	c := aedmz.GetContext(r)
	parentKey := aedmz.NewKey("ContentNamespace", namespace, nil)
	k := aedmz.NewKey("ContentEntry", hashKey, parentKey)
	e := new(ContentEntry)
	if err := aedmz.Get(c, k, e); err != nil {
		c.Errorf("Failed to find object: %s", err)
		return
	}
	if e.ExpandedSize != -1 {
		c.Warningf("Was already verified: %d", e.ExpandedSize)
		return
	}
	if len(e.Content) != 0 {
		c.Errorf("Should not be called with inline content: %d", len(e.Content))
		return
	}

	// Get GS file size.
	s := settings(c)
	bucket := s.GSBucket
	gsFilepath := e.gsFilepath(k)
	fileInfo, err := getFileInfo(c, bucket, gsFilepath)
	if err != nil {
		// According to the docs, GS is read-after-write consistent, so a file is
		// missing only if it wasn't stored at all or it was deleted, in any case
		// it's not a valid ContentEntry.
		c.Errorf("No such GS file")
		_ = e.purge(c, k, bucket)
		return
	}

	// Expected stored length and actual length should match.
	if fileInfo.Size != uint64(e.Size) {
		c.Errorf("Bad GS file: expected size is %d, actual size is %d", e.Size, fileInfo.Size)
		_ = e.purge(c, k, bucket)
		return
	}

	// TODO(maruel): saveToMemcache := e.Size <= MaxMemcacheIsolated && e.IsIsolated

	// Start a loop where it reads the data in block.
	stream, err := readFile(c, bucket, gsFilepath)
	if err != nil {
		c.Errorf("Failed to read GS file")
		_ = e.purge(c, k, bucket)
		return
	}
	hexDigest, expandedSize, err := expandAndHashContent(stream, namespace)
	if err != nil {
		c.Errorf("Expansion failure: %s", err)
		_ = e.purge(c, k, bucket)
		return
	}
	if hexDigest != hashKey {
		c.Errorf("Hash do not match, got %s (%d bytes expanded)", hexDigest, expandedSize)
		_ = e.purge(c, k, bucket)
		return
	}
	//if saveToMemcache {
	//	// Wraps stream with a generator that accumulates the data.
	//	stream = Accumulator(stream)
	//}

	// Verified. Data matches the hash.
	e.ExpandedSize = expandedSize
	e.LastAccess = time.Now().UTC()
	if _, err = aedmz.Put(c, k, e); err != nil {
		// This needs to be retried.
		sendError(c, w, 503, "Failed to update ContentEntry")
		return
	}
	c.Infof("%d bytes (%d bytes expanded) verified", e.Size, e.ExpandedSize)
	//if save_to_memcache {
	//	// TODO(maruel): save_in_memcache(namespace, hash_key, "".join(stream.accumulated))
	//}
}

type handshakeRequest struct {
	ProtocolVersion  string `json:"protocol_version"`
	ClientAppVersion string `json:"client_app_version"`
	Fetcher          bool   `json:"fetcher"`
	Pusher           bool   `json:"pusher"`
}

type handshakeResponse struct {
	ProtocolVersion  string `json:"protocol_version"`
	ServerAppVersion string `json:"server_app_version"`
	AccessToken      string `json:"access_token,omitempty"`
	Error            string `json:"error,omitempty"`
}

// handshakeHandler returns access token, version and capabilities of the
// server to the http client.
//
// Request body is a JSON dict of handshakeRequest.
// Response body is a JSON dict of handshakeResponse.
func handshakeHandler(w http.ResponseWriter, r *http.Request) {
	// This handler is called to get the token, there's nothing to enforce yet.
	c := aedmz.GetContext(r)
	h := &handshakeRequest{Pusher: true, Fetcher: true}
	reply := &handshakeResponse{ProtocolVersion: IsolateProtocolVersion, ServerAppVersion: c.AppVersion()}
	if err := json.NewDecoder(r.Body).Decode(h); err != nil {
		reply.Error = fmt.Sprintf("Invalid body for handshake call.\nError: %s", err)
		w.WriteHeader(400)
		sendJSON(w, reply)
		return
	}
	// TODO(maruel): Assert h.ProtocolVersion is 1.x.
	accessID := GetAccessID(c, r)
	if accessID == "" {
		sendError(c, w, http.StatusUnauthorized, "Please login first")
		return
	}
	// This access token will be used to validate each subsequent request.
	t := map[string]string{"v": h.ProtocolVersion}
	accessToken := generateToken(c, accessID, t)
	reply.AccessToken = accessToken
	sendJSON(w, reply)
}

// retrieveContentHandler is used by the http client to retrieve content by its
// SHA-1 hash |hash_key|.
//
// Can produce 5 types of responses:
//   * HTTP 200: the content is in response body as octet-stream.
//   * HTTP 206: partial content is in response body.
//   * HTTP 302: http redirect to a file with the content.
//   * HTTP 404: content is not available, response body is a error message.
//   * HTTP 416: requested byte range can not be satisfied.
func retrieveContentHandler(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	namespace := vars["namespace"]
	hashKey := vars["hashkey"]

	// TODO(maruel): Support Range requests, see src/pkg/net/http/fs.go
	// TODO(maruel): Add memcache support.
	// memcache_entry := c.CacheGet(hashKey, 'table_%s' % namespace)
	// if memcache_entry is not None {
	// 	sendData(c, memcache_entry, filename=hash_key, offset=offset)
	// 	stats.log(stats.RETURN, len(memcache_entry) - offset, 'memcache')
	// 	return
	// }
	c := aedmz.GetContext(r)
	k := aedmz.NewKey("ContentEntry", hashKey, aedmz.NewKey("ContentNamespace", namespace, nil))
	e := new(ContentEntry)
	if err := aedmz.Get(c, k, e); err != nil {
		sendError(c, w, http.StatusNotFound, "Failed to find object: %s", err)
		return
	}
	if e.GSFilename == "" {
		sendData(c, w, e.Content, hashKey, 0)
		// TODO(maruel): Add.
		//stats.log(stats.RETURN, len(entry.content) - offset, 'inline')
		return
	}

	// Generate signed download URL.
	s := settings(c)
	// TODO(maruel): The GS object may not exist anymore, the ContentEntry entity
	// should be deleted.

	signer := newURLSigner(c, s.GSBucket, s.GSClientIDEmail, s.GSPrivateKey)
	// Redirect client to this URL. If 'Range' header is used, client will
	// correctly pass it to Google Storage to fetch only subrange of file,
	// so update stats accordingly.
	// TODO(maruel): Make it permanent?
	f := fmt.Sprintf("%s/%s", namespace, e.GSFilename)
	destURL := signer.GetDownloadURL(f, 0)
	c.Infof("-> %s", destURL)
	http.Redirect(w, r, destURL, http.StatusFound)
	// TODO(maruel): Add.
	//stats.log(stats.RETURN, entry.size - offset, 'GS; %s' % e.GSFilename)
}

// generateSignature generates hex encoded HMAC-SHA1 signature for given set of
// parameters.
//
// Used by PreUploadContentHandlerGS to sign store URLs and by
// StoreContentHandlerGS to validate them.
func generateSignature(secretKey, httpVerb, expiration, namespace, hashKey, itemSize, isIsolated, uploadedToGS string) string {
	s := []string{httpVerb, expiration, namespace, hashKey, itemSize, isIsolated, uploadedToGS}
	// Note that it is creating the signature on the base64 encoded key, not the decoded one.
	mac := hmac.New(sha1.New, []byte(secretKey))
	_, _ = mac.Write([]byte(strings.Join(s, "\n")))
	return hex.EncodeToString(mac.Sum(nil))
}

func generateStoreURLQueryParams(secretKey string, entry *FileEntry, namespace, httpVerb string, uploadedToGS bool, expiration time.Duration, token string) string {
	// Data that goes into request parameters and signature.
	expirationStr := strconv.FormatInt(time.Now().UTC().Add(expiration).Unix(), 10)
	sizeStr := strconv.FormatInt(entry.Size, 10)
	isIsolatedStr := strconv.Itoa(entry.IsIsolated)
	uploadedToGSStr := "0"
	if uploadedToGS {
		uploadedToGSStr = "1"
	}

	// Generate signature.
	sig := generateSignature(secretKey, httpVerb, expirationStr, namespace, entry.HexDigest, sizeStr, isIsolatedStr, uploadedToGSStr)

	// Construct url with query parameters, reuse auth token.
	params := url.Values{}
	params.Set("g", uploadedToGSStr)
	params.Set("i", isIsolatedStr)
	params.Set("s", sizeStr)
	params.Set("sig", sig)
	params.Set("token", token)
	params.Set("x", expirationStr)
	return params.Encode()
}

// generatePushURLs generates a pair of URLs to be used by clients to upload an
// item.
//
// URL's being generated are 'upload URL' and 'finalize URL'. Client uploads
// an item to upload URL (via PUT request) and then POST status of the upload
// to a finalize URL.
//
// Finalize URL may be optional (it's None in that case).
func generatePushURLs(router *mux.Router, r *http.Request, privateKey string, s *urlSigner, entry *FileEntry, namespace, token string) []string {
	url, err := router.Get("store").URL("namespace", namespace, "hashkey", entry.HexDigest)
	if err != nil {
		panic(fmt.Sprintf("Internal error: %s", err))
	}
	url.Host = r.Host
	// TODO(maruel): Hack.
	if r.TLS == nil {
		url.Scheme = "http"
	} else {
		url.Scheme = "https"
	}
	if shouldPushToGS(entry) {
		// Store larger stuff in Google Storage.
		uploadURL := s.GetUploadURL(fmt.Sprintf("%s/%s", namespace, entry.HexDigest), DefaultLinkExpiration, "application/octet-stream", nil)
		url.RawQuery = generateStoreURLQueryParams(privateKey, entry, namespace, "POST", true, DefaultLinkExpiration, token)
		return []string{uploadURL, url.String()}
	}
	// Store smallish entries and *.isolated in Datastore directly.
	url.RawQuery = generateStoreURLQueryParams(privateKey, entry, namespace, "PUT", false, DefaultLinkExpiration, token)
	return []string{url.String(), ""}
}

// preUploadContentHandler checks for entries existence and generates upload
// URLs.
//
// Request body is a []FileEntry.
// Response is a [][]string. Each item is either:
//   * If an entry is missing: a list with two URLs - URL to upload a file to,
//     and URL to call when upload is done (can be null).
//   * If entry is already present: null.
//
// For instance:
// [
//     ["<upload url>", "<finalize url>"],
//     null,
//     null,
//     ["<upload url>", null],
//     null,
//     ...
// ]
func preUploadContentHandler(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	namespace := vars["namespace"]

	c := aedmz.GetContext(r)
	token := r.Form.Get("token")
	if len(token) == 0 {
		sendError(c, w, 400, "Missing token")
		return
	}

	p := make([]FileEntry, 100)
	if err := json.NewDecoder(r.Body).Decode(&p); err != nil {
		sendError(c, w, 400, "Bad /pre-upload request: %s", err)
		return
	}
	re := regexp.MustCompile(fmt.Sprintf("[a-f0-9A-z]{%d}", getHash(namespace).Size()*2))
	for _, v := range p {
		if !v.IsValid(re) {
			sendError(c, w, 400, "Bad /pre-upload request: bad item")
			return
		}
	}

	// Lookup for presence.
	parentKey := aedmz.NewKey("ContentNamespace", namespace, nil)
	ch := make(chan int)
	for i, v := range p {
		// Sends i+1 if the object is missing and -(i+1) if the object is present.
		go func() {
			k := aedmz.NewKey("ContentEntry", v.HexDigest, parentKey)
			if err := aedmz.Get(c, k, &v); err != nil {
				ch <- i + 1
			} else {
				ch <- -1 - i
			}
		}()
	}
	s := settings(c)
	router, ok := gorillaContext.Get(r, routerKey).(*mux.Router)
	if !ok {
		sendError(c, w, 500, "Internal error")
		return
	}
	signer := newURLSigner(c, s.GSBucket, s.GSClientIDEmail, s.GSPrivateKey)
	existing := make([]string, 0)
	count := len(p)
	reply := make([][]string, count)
	retrieved := 0
	for {
		select {
		case x := <-ch:
			if x > 0 {
				// Missing.
				x--
				reply[x] = generatePushURLs(router, r, s.GSPrivateKey, signer, &p[x], namespace, token)
			} else {
				// Present.
				x++
				existing = append(existing, p[x].HexDigest)
			}
			retrieved++
		}
		if retrieved == count {
			break
		}
	}
	sendJSON(w, reply)

	// Log stats, enqueue tagging task that updates last access time.
	//stats.log(stats.LOOKUP, len(entries), len(existing))
	if len(existing) != 0 {
		// Ignore errors in a call below. They happen when task queue service has a
		// bad time and doesn't accept tagging tasks. We don't want isolate
		// server's reliability to depend on task queue service health. An ignored
		// error here means there's a chance some entry might be deleted sooner
		// than it should.
		//url := fmt.Sprintf("/internal/taskqueue/tag/%s/%s", namespace, time.Now().UTC().Format("2006/01/02"))
		//payload = strings.Join(binascii.unhexlify(e) for e in existing)
		//return enqueue_task(url, "tag", payload=payload)
	}
}

// storeContentHandler creates a ContentEntry Datastore entity for an uploaded
// file.
//
// Clients usually do not call this handler explicitly. Signed URL to it
// is returned in /pre-upload call.
//
// This handler is called in two ways:
//   * As a POST request to finalize a file already uploaded to GS. Request
//     body is empty in that case.
//   * As a PUT request to upload an actual data and create ContentEntry in one
//     call. Request body contains octet-stream with entry's data.
//
// In either case query parameters define details of new content entry:
//   g - 1 if it was previously uploaded to GS.
//   i - 1 if it its *.isolated file.
//   s - size of the uncompressed file.
//   x - URL signature expiration timestamp.
//   sig - signature of request parameters, to verify they are not tampered with.
//
// Can produce 3 types of responses:
//   * HTTP 200: success, entry is created or existed before, response body is
//     a json dict with information about new entry.
//   * HTTP 400: fatal error, retrying request won't fix it, response body is
//     a error message.
//   * HTTP 503: transient error, request should be retried by client, response
//     body is a error message.
//
// In case of HTTP 200, body is a JSON dict:
// {
//     'entry': {<details about the entry>}
// }
//
// POST is used when finalizing upload to GS.
// PUT is used when uploading directly to datastore via this handler.
func storeContentHandler(w http.ResponseWriter, r *http.Request) {
	vars := mux.Vars(r)
	namespace := vars["namespace"]
	hashKey := vars["hashkey"]

	_ = r.ParseForm()
	expirationStr := r.Form.Get("x")
	itemSizeStr := r.Form.Get("s")
	isIsolatedStr := r.Form.Get("i")
	uploadedToGSStr := r.Form.Get("g")
	signature := r.Form.Get("sig")

	// Build correct signature.
	c := aedmz.GetContext(r)
	expectedSig := generateSignature(settings(c).GSPrivateKey, r.Method, expirationStr, namespace, hashKey, itemSizeStr, isIsolatedStr, uploadedToGSStr)
	// Verify signature is correct.
	if subtle.ConstantTimeCompare([]byte(signature), []byte(expectedSig)) != 1 {
		sendError(c, w, 400, "Incorrect signature.")
		return
	}

	// Convert parameters from strings back to something useful.
	// It can't fail since matching signature means it was us who generated
	// this strings in a first place.
	expirationTS, err := strconv.ParseInt(expirationStr, 10, 64)
	if err != nil {
		sendError(c, w, 400, "Incorrect parameter.")
		return
	}
	itemSize, err := strconv.ParseInt(itemSizeStr, 10, 64)
	if err != nil {
		sendError(c, w, 400, "Incorrect parameter.")
		return
	}
	isIsolated, err := strconv.ParseBool(isIsolatedStr)
	if err != nil {
		sendError(c, w, 400, "Incorrect parameter.")
		return
	}
	uploadedToGS, err := strconv.ParseBool(uploadedToGSStr)
	if err != nil {
		sendError(c, w, 400, "Incorrect parameter.")
		return
	}
	// Verify signature is not yet expired.
	if time.Now().UTC().Unix() > expirationTS {
		sendError(c, w, 400, "Expired signature.")
		return
	}
	var content []byte
	if uploadedToGS {
		// GS upload finalization uses empty POST body.
		if r.Method != "POST" {
			sendError(c, w, 400, "Expecting POST.")
			return
		}
		if r.ContentLength != 0 {
			sendError(c, w, 400, "Expecting empty POST.")
			return
		}
	} else {
		// Datastore upload uses PUT.
		if r.Method != "PUT" {
			sendError(c, w, 400, "Expecting PUT.")
			return
		}
		if content, err = ioutil.ReadAll(r.Body); err != nil {
			sendError(c, w, 500, "I need some Body")
			return
		}
	}

	// Info about corresponding GS entry (if it exists).
	gsBucket := settings(c).GSBucket
	gsFilepath := fmt.Sprintf("%s/%s", namespace, hashKey)
	// To be populated below.
	var compressedSize int64
	needsVerification := true
	if content != nil {
		// Verify the data while at it since it's already in memory but before
		// storing it in memcache and datastore.
		compressedSize = int64(len(content))
		// Verify advertised hash matches the data.
		hexDigest, expandedSize, err := expandAndHashContent(ioutil.NopCloser(bytes.NewReader(content)), namespace)
		if err != nil {
			sendError(c, w, 400, "Expansion failure: %s", err)
			return
		}
		if hexDigest != hashKey {
			sendError(c, w, 400, "Hash and data do not match, %d bytes (%d bytes expanded)", len(content), expandedSize)
			return
		}
		// It must match.
		if expandedSize != itemSize {
			sendError(c, w, 400, "Advertised data length (%d) and actual data length (%d) do not match", itemSize, expandedSize)
			return
		}
		//return self.send_error('Inline verification failed.\n%s' % err)
		// Successfully verified!
		needsVerification = false
	} else {
		// Fetch size of the stored file.
		fileInfo, err := getFileInfo(c, gsBucket, gsFilepath)
		if err != nil {
			sendError(c, w, 400, "File should be in Google Storage.\nFile is '%s': %s", gsFilepath, err)
			return
		}
		compressedSize = int64(fileInfo.Size)
	}
	// Data is here and it's too large for DS, so put it in GS.
	if content != nil && len(content) >= MinSizeForGS {
		if err := writeFile(c, gsBucket, gsFilepath, content); err != nil {
			// Returns 503 so the client automatically retries.
			sendError(c, w, 503, "Unable to save the content to GS.")
			return
		}
		// It's now in GS.
		uploadedToGS = true
	}

	// Can create entity now, everything appears to be legit.
	entry, err := createEntry(c, namespace, hashKey)
	if err != nil {
		sendError(c, w, 503, "%s", err)
		return
	}
	if entry == nil {
		sendJSON(w, map[string]map[string]string{"entry": {}})
		// TODO(maruel): stats.log(stats.DUPE, compressedSize, 'inline')
		return
	}

	// If it's not in GS then put it inline.
	if !uploadedToGS {
		//assert content is not None and len(content) < MinSizeForGS
		entry.Content = content
	} else {
		// Start saving Datastore entry.
		entry.GSFilename = hashKey
	}
	entry.IsIsolated = isIsolated
	entry.Size = compressedSize
	if !needsVerification {
		entry.ExpandedSize = itemSize
	}
	entry.LastAccess = time.Now().UTC()
	k := aedmz.NewKey("ContentEntry", hashKey, aedmz.NewKey("ContentNamespace", namespace, nil))
	if _, err = aedmz.Put(c, k, entry); err != nil {
		sendError(c, w, 503, "Write failure: %s", err)
		return
	}
	c.Infof("Saved %#v", entry)

	// Start saving *.isolated into memcache iff its content is available and
	// it's not in Datastore: there's no point in saving inline blobs in memcache
	// because ndb already memcaches them.
	// TODO(maruel): DB automatic cache. Eh.
	if content != nil && entry.Content == nil && entry.IsIsolated && entry.Size <= MaxMemcacheIsolated {
		// TODO(maruel): futures.append(save_in_memcache(namespace, hash_key, content, async=True))
	}
	// Log stats.
	// TODO(maruel): where = 'GS; ' + entry.filename if entry.filename else 'inline'

	// Trigger a verification task for files in the GS.
	if needsVerification {
		url := fmt.Sprintf("/internal/taskqueue/verify/%s/%s", namespace, hashKey)
		if err := c.TaskEnqueue(url, "verify", nil); err != nil {
			// TODO(vadimsh): Don't fail whole request here, because several RPCs are
			// required to roll it back and there isn't much time left
			// (after DeadlineExceededError is already caught) to perform them.
			// AppEngine gives less then a second to handle DeadlineExceededError.
			// It's unreliable. We need some other mechanism that can detect
			// unverified entities and launch verification tasks, for instance
			// a datastore index on 'is_verified' boolean field and a cron task that
			// verifies all unverified entities.
			c.Errorf("Unable to add task to verify uploaded content.")
		}
	}
	// TODO(vadimsh): Fill in details about the entry, such as expiration time.
	sendJSON(w, map[string]map[string]string{"entry": {}})
	// TODO(maruel): stats.log(stats.STORE, entry.size, where)
}

// warmUpHandler makes sure settings are loaded and the templates will be
// compiled on startup.
func warmUpHandler(w http.ResponseWriter, r *http.Request) {
	c := aedmz.GetContext(r)
	settings(c)
	_, _ = w.Write([]byte("Warmed up"))
}

func rootHandler(w http.ResponseWriter, r *http.Request) {
	SendTemplate(w, "root.html", 0)
}

// handle adds a route 'path' to the router 'r' named 'name' using 'handler',
// restricted to specified 'methods'.
func handle(r *mux.Router, path string, name string, handler http.HandlerFunc, methods ...string) {
	r.HandleFunc(path, handler).Name(name).Methods(methods...)
}

// SetupHandlers adds all the isolate server routes to the web server router.
func SetupHandlers(router *http.ServeMux, app aedmz.AppContext) {
	// Namespace can be letters, numbers and '-'.
	// Do not enforce a length limit to support different hashing algorithm. This
	// should represent a valid hex value.
	namespace := "{namespace:[a-z0-9A-Z\\-]+}"
	hashKey := "{hashkey:[a-f0-9]{4,}}"
	namespaceHash := namespace + "/" + hashKey

	// Route through Gorilla mux for native regexp and named route support.
	r := mux.NewRouter()
	{
		content := r.PathPrefix("/content-gs").Subrouter()
		handle(content, "/handshake", "handshake", handshakeHandler, "POST")
		handle(content, "/pre-upload/"+namespace, "pre-upload", ACL(preUploadContentHandler), "POST")
		handle(content, "/retrieve/"+namespaceHash, "retrieve", ACL(retrieveContentHandler), "GET", "HEAD")
		handle(content, "/store/"+namespaceHash, "store", ACL(storeContentHandler), "POST", "PUT")
	}
	handle(r, "/internal/taskqueue/verify/"+namespaceHash, "", internalVerifyWorkerHandler, "POST")
	handle(r, "/_ah/warmup", "", warmUpHandler, "GET")
	handle(r, "/", "root", rootHandler, "GET")

	h := app.InjectContext(r.ServeHTTP)

	// Set our router as the sole handler to 'router'.
	router.HandleFunc("/", func(w http.ResponseWriter, req *http.Request) {
		gorillaContext.Set(req, routerKey, r)
		h.ServeHTTP(w, req)
	})
}
